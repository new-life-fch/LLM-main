#!/usr/bin/env python3
"""
CausalEditor é¢„è®¡ç®—è„šæœ¬
ç”¨äºæ‰§è¡ŒçŸ¥è¯†æå–å’Œæ¿€æ´»æŒ‡çº¹æ„å»ºçš„å®Œæ•´æµç¨‹

ä½¿ç”¨ç¤ºä¾‹:
python scripts/precompute_causal_knowledge.py \
    --model-name meta-llama/Llama-2-7b-hf \
    --output-dir ./precomputed_data/llama2-7b \
    --knowledge-type wikidata \
    --limit 10000 \
    --device cuda \
    --batch-size 8
"""

import argparse
import json
import logging
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from causal_editor.precompute.precompute_pipeline import PrecomputePipeline


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="CausalEditor é¢„è®¡ç®—è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

1. ä»Wikidataæå–10000ä¸ªä¸‰å…ƒç»„å¹¶æ„å»ºæ¿€æ´»æŒ‡çº¹:
   python scripts/precompute_causal_knowledge.py \\
       --model-name meta-llama/Llama-2-7b-hf \\
       --output-dir ./precomputed_data/llama2-7b \\
       --knowledge-type wikidata \\
       --limit 10000

2. ä»CSVæ–‡ä»¶æ„å»ºæ¿€æ´»æŒ‡çº¹:
   python scripts/precompute_causal_knowledge.py \\
       --model-name mistralai/Mistral-7B-v0.1 \\
       --output-dir ./precomputed_data/mistral-7b \\
       --knowledge-type csv \\
       --csv-path ./data/knowledge_triplets.csv \\
       --limit 5000

3. åªåœ¨ç‰¹å®šå±‚æ„å»ºæŒ‡çº¹:
   python scripts/precompute_causal_knowledge.py \\
       --model-name meta-llama/Llama-2-7b-hf \\
       --target-layers 10 11 12 13 14 15 \\
       --limit 10000
        """
    )
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--model-name",
        type=str,
        required=True,
        help="æ¨¡å‹åç§°æˆ–è·¯å¾„ (ä¾‹å¦‚: meta-llama/Llama-2-7b-hf)"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./precomputed_data",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ./precomputed_data)"
    )
    
    # çŸ¥è¯†æå–å‚æ•°
    parser.add_argument(
        "--knowledge-type",
        type=str,
        choices=["wikidata", "csv"],
        default="wikidata",
        help="çŸ¥è¯†æºç±»å‹ (é»˜è®¤: wikidata)"
    )
    
    parser.add_argument(
        "--csv-path",
        type=str,
        help="CSVæ–‡ä»¶è·¯å¾„ (å½“knowledge-typeä¸ºcsvæ—¶å¿…éœ€)"
    )
    
    parser.add_argument(
        "--limit",
        type=int,
        default=10000,
        help="æå–çš„ä¸‰å…ƒç»„æ•°é‡é™åˆ¶ (é»˜è®¤: 10000)"
    )
    
    parser.add_argument(
        "--rate-limit",
        type=float,
        default=1.0,
        help="APIè¯·æ±‚é€Ÿç‡é™åˆ¶ï¼Œç§’ (é»˜è®¤: 1.0)"
    )
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è®¡ç®—è®¾å¤‡ (é»˜è®¤: cuda)"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 8)"
    )
    
    parser.add_argument(
        "--target-layers",
        type=int,
        nargs="+",
        help="ç›®æ ‡å±‚åˆ—è¡¨ (ä¾‹å¦‚: --target-layers 10 11 12). ä¸æŒ‡å®šåˆ™ä½¿ç”¨æ‰€æœ‰å±‚"
    )
    
    # åŠŸèƒ½é€‰é¡¹
    parser.add_argument(
        "--incremental",
        action="store_true",
        help="å¢é‡æ›´æ–°ç°æœ‰æ•°æ®åº“"
    )
    
    parser.add_argument(
        "--existing-db-path",
        type=str,
        help="ç°æœ‰æ•°æ®åº“è·¯å¾„ (å¢é‡æ¨¡å¼å¿…éœ€)"
    )
    
    parser.add_argument(
        "--validate-only",
        action="store_true",
        help="ä»…éªŒè¯ç°æœ‰æ•°æ®åº“"
    )
    
    parser.add_argument(
        "--cleanup-cache",
        action="store_true",
        help="å®Œæˆåæ¸…ç†ç¼“å­˜"
    )
    
    parser.add_argument(
        "--config-file",
        type=str,
        help="é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)"
    )
    
    parser.add_argument(
        "--generate-config",
        action="store_true",
        help="ç”Ÿæˆé…ç½®æ¨¡æ¿æ–‡ä»¶"
    )
    
    # æ—¥å¿—å‚æ•°
    parser.add_argument(
        "--log-level",
        type=str,
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)"
    )
    
    return parser.parse_args()


def setup_logging(log_level: str):
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )


def load_config(config_file: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)


def generate_config_template(output_path: str):
    """ç”Ÿæˆé…ç½®æ¨¡æ¿"""
    pipeline = PrecomputePipeline("dummy", "dummy")
    template = pipeline.get_config_template()
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(template, f, indent=2, ensure_ascii=False)
    
    print(f"é…ç½®æ¨¡æ¿å·²ç”Ÿæˆ: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    
    # ç”Ÿæˆé…ç½®æ¨¡æ¿
    if args.generate_config:
        config_path = args.config_file or "causal_editor_config.json"
        generate_config_template(config_path)
        return
    
    # éªŒè¯å‚æ•°
    if args.knowledge_type == "csv" and not args.csv_path:
        logging.error("ä½¿ç”¨CSVçŸ¥è¯†æºæ—¶å¿…é¡»æŒ‡å®š --csv-path")
        sys.exit(1)
    
    if args.incremental and not args.existing_db_path:
        logging.error("å¢é‡æ¨¡å¼éœ€è¦æŒ‡å®š --existing-db-path")
        sys.exit(1)
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
    if args.config_file:
        config = load_config(args.config_file)
        # å‘½ä»¤è¡Œå‚æ•°ä¼šè¦†ç›–é…ç½®æ–‡ä»¶
        model_name = args.model_name or config.get("model", {}).get("name")
        device = args.device or config.get("model", {}).get("device", "cuda")
        batch_size = args.batch_size or config.get("model", {}).get("batch_size", 8)
        target_layers = args.target_layers or config.get("fingerprint", {}).get("target_layers")
        limit = args.limit or config.get("limits", {}).get("max_triplets", 10000)
    else:
        model_name = args.model_name
        device = args.device
        batch_size = args.batch_size
        target_layers = args.target_layers
        limit = args.limit
    
    # åˆå§‹åŒ–æµæ°´çº¿
    pipeline = PrecomputePipeline(
        model_name=model_name,
        output_dir=args.output_dir,
        device=device,
        batch_size=batch_size
    )
    
    try:
        # ä»…éªŒè¯æ¨¡å¼
        if args.validate_only:
            if not args.existing_db_path:
                logging.error("éªŒè¯æ¨¡å¼éœ€è¦æŒ‡å®šæ•°æ®åº“è·¯å¾„")
                sys.exit(1)
            
            logging.info("å¼€å§‹éªŒè¯æ•°æ®åº“...")
            result = pipeline.validate_database(args.existing_db_path)
            
            print("\néªŒè¯ç»“æœ:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
            if result['status'] == 'success':
                logging.info("æ•°æ®åº“éªŒè¯é€šè¿‡")
            else:
                logging.warning(f"æ•°æ®åº“éªŒè¯å¤±è´¥: {result.get('message', 'Unknown error')}")
            
            return
        
        # å‡†å¤‡çŸ¥è¯†æå–é…ç½®
        knowledge_config = {
            'type': args.knowledge_type,
            'rate_limit': args.rate_limit
        }
        
        if args.knowledge_type == 'csv':
            knowledge_config['csv_path'] = args.csv_path
        
        # è¿è¡Œæµæ°´çº¿
        if args.incremental:
            logging.info("å¼€å§‹å¢é‡æ›´æ–°...")
            db_path = pipeline.run_incremental_update(
                new_knowledge_config=knowledge_config,
                existing_db_path=args.existing_db_path,
                limit=limit
            )
        else:
            logging.info("å¼€å§‹å®Œæ•´é¢„è®¡ç®—æµæ°´çº¿...")
            db_path = pipeline.run_full_pipeline(
                knowledge_config=knowledge_config,
                limit=limit,
                target_layers=target_layers
            )
        
        # éªŒè¯ç»“æœ
        logging.info("éªŒè¯ç”Ÿæˆçš„æ•°æ®åº“...")
        validation_result = pipeline.validate_database(db_path)
        
        if validation_result['status'] == 'success':
            logging.info("æ•°æ®åº“æ„å»ºå¹¶éªŒè¯æˆåŠŸ!")
            print(f"\nâœ… é¢„è®¡ç®—å®Œæˆ!")
            print(f"ğŸ“Š å‘é‡æ•°æ®åº“è·¯å¾„: {db_path}")
            print(f"ğŸ“ˆ æ€»å‘é‡æ•°: {validation_result.get('total_vectors', 0)}")
            print(f"ğŸ“ å‘é‡ç»´åº¦: {validation_result.get('dimension', 0)}")
        else:
            logging.error(f"æ•°æ®åº“éªŒè¯å¤±è´¥: {validation_result.get('message', 'Unknown error')}")
            sys.exit(1)
        
        # æ¸…ç†ç¼“å­˜
        if args.cleanup_cache:
            logging.info("æ¸…ç†ç¼“å­˜...")
            pipeline.cleanup_cache()
        
        logging.info("é¢„è®¡ç®—æµæ°´çº¿æ‰§è¡Œå®Œæˆ!")
        
    except KeyboardInterrupt:
        logging.info("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        logging.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 