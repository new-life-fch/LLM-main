# CausalEditor: å±‚çº§åŒ–å› æœæº¯æºä¸åäº‹å®ç¼–è¾‘

## é¡¹ç›®æ¦‚è¿°

CausalEditor æ˜¯ä¸€ç§æ–°é¢–çš„LLMå¹»è§‰ä¿®æ­£æ–¹æ³•ï¼Œé€šè¿‡**å±‚çº§åŒ–å› æœæº¯æº**å’Œ**åäº‹å®ç¼–è¾‘**æŠ€æœ¯ï¼Œåœ¨æ¨ç†æ—¶ç²¾ç¡®ä¿®æ­£å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„ç»Ÿä¸€çœŸå®æ€§æ–¹å‘ç¼–è¾‘ï¼ˆå¦‚TruthXï¼‰ä¸åŒï¼ŒCausalEditoré‡‡ç”¨é¢„è®¡ç®—çš„å› æœçŸ¥è¯†å›¾è°±æ¿€æ´»ç¼–ç ï¼Œå®ç°"å¤–ç§‘æ‰‹æœ¯å¼"çš„ç²¾ç¡®ç¼–è¾‘ã€‚

## æ ¸å¿ƒæ€æƒ³

### å› æœæ–­è£‚æ£€æµ‹
- **é—®é¢˜å‡è®¾**ï¼šLLMå¹»è§‰å¾€å¾€æºäºä¿¡æ¯æµåœ¨ç‰¹å®šå±‚ã€ç‰¹å®šç¥ç»å…ƒä¸Šçš„"å› æœæ–­è£‚"
- **è§£å†³æ–¹æ¡ˆ**ï¼šé€šè¿‡é¢„è®¡ç®—çš„çŸ¥è¯†æ¿€æ´»æŒ‡çº¹ï¼Œå®æ—¶æ£€æµ‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å› æœå†²çª

### åäº‹å®ç¼–è¾‘
- **ç²¾ç¡®ç¼–è¾‘**ï¼šä¸æ˜¯åº”ç”¨ç»Ÿä¸€çš„çœŸå®æ€§æ–¹å‘ï¼Œè€Œæ˜¯åŸºäºæ£€ç´¢åˆ°çš„æ­£ç¡®çŸ¥è¯†è¿›è¡Œç²¾ç¡®çš„åäº‹å®ç¼–è¾‘
- **æœ€å°å¹²é¢„**ï¼šåªåœ¨æ£€æµ‹åˆ°å†²çªæ—¶è¿›è¡Œç¼–è¾‘ï¼Œæœ€å¤§åŒ–ä¿ç•™åŸå§‹æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¬¬1æ­¥ï¼šç¯å¢ƒå‡†å¤‡

```bash
# è§£å†³ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
python fix_compatibility.py

# æˆ–è€…æ‰‹åŠ¨åˆ›å»ºæ–°ç¯å¢ƒï¼ˆæ¨èï¼‰
conda create -n causal-editor python=3.10 -y
conda activate causal-editor
conda install pytorch==2.0.0 torchvision==0.15.0 pytorch-cuda=11.8 -c pytorch -c nvidia -y
conda install transformers==4.30.0 faiss-cpu -c conda-forge -y
pip install numpy pandas requests tqdm scikit-learn matplotlib
```

### ç¬¬2æ­¥ï¼šæµ‹è¯•åŸºç¡€åŠŸèƒ½

```bash
# æµ‹è¯•æ ¸å¿ƒç»„ä»¶
python test_basic_functionality.py
```

é¢„æœŸè¾“å‡ºï¼š
```
âœ… æ ¸å¿ƒç»„ä»¶å¯¼å…¥æˆåŠŸ
âœ… é¢„è®¡ç®—ç»„ä»¶å¯¼å…¥æˆåŠŸ
âœ… åŸºç¡€ä¾èµ–: PyTorch, Transformers, FAISSç­‰
âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ
âœ… æµ‹è¯•æ•°æ®å·²åˆ›å»º
```

### ç¬¬3æ­¥ï¼šé¢„è®¡ç®—çŸ¥è¯†æŒ‡çº¹

```bash
# ä»Wikidataæå–çŸ¥è¯†å¹¶æ„å»ºæ¿€æ´»æŒ‡çº¹
python scripts/precompute_causal_knowledge.py \
    --model-name meta-llama/Llama-2-7b-hf \
    --output-dir ./precomputed_data/llama2-7b \
    --knowledge-type wikidata \
    --limit 10000 \
    --device cuda \
    --batch-size 8
```

### ç¬¬4æ­¥ï¼šä½¿ç”¨CausalEditor

```python
from modeling_llms.modeling_llama_causal import CausalLlamaForCausalLM
from transformers import AutoTokenizer
import torch

# åŠ è½½é›†æˆCausalEditorçš„æ¨¡å‹
model = CausalLlamaForCausalLM.from_pretrained_with_causal_editor(
    model_name_or_path="meta-llama/Llama-2-7b-hf",
    vector_db_path="./precomputed_data/llama2-7b/vector_database",
    edit_strength=1.0,
    top_layers=10,
    device="cuda"
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
model.set_tokenizer(tokenizer)

# ç”Ÿæˆæ— å¹»è§‰æ–‡æœ¬
question = "What year did Einstein publish his paper on the photoelectric effect?"
inputs = tokenizer(question, return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        do_sample=True,
        temperature=0.7
    )

answer = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
print(f"ç­”æ¡ˆ: {answer}")
```

### ç¬¬5æ­¥ï¼šè¯„ä¼°æ•ˆæœ

```bash
# åœ¨TruthfulQAä¸Šè¯„ä¼°
python scripts/evaluate_truthfulqa_causal.py \
    --model-path meta-llama/Llama-2-7b-hf \
    --vector-db-path ./precomputed_data/llama2-7b/vector_database \
    --output-dir ./results/evaluation \
    --mode both \
    --edit-strength 1.0 \
    --top-layers 10
```

## ä¸»è¦ç‰¹æ€§

### ğŸ¯ ç²¾ç¡®ç¼–è¾‘
- **å¤–ç§‘æ‰‹æœ¯å¼ä¿®æ­£**ï¼šåªä¿®æ”¹é”™è¯¯çš„å…³é”®äº‹å®ï¼Œä¿æŒè¯­è¨€æµç•…æ€§
- **å®æ—¶æ£€æµ‹**ï¼šæ¨ç†æ—¶åŠ¨æ€æ£€æµ‹å› æœå†²çª
- **æœ€å°å‰¯ä½œç”¨**ï¼šç›¸æ¯”RAGç­‰æ–¹æ³•ï¼Œå¯¹åŸæ–‡æ”¹åŠ¨æœ€å°

### âš¡ é«˜æ•ˆæ¨ç†
- **ä½å»¶è¿Ÿ**ï¼šæ¨ç†æ—¶é—´å¢åŠ <10%
- **å•å¡å‹å¥½**ï¼šæ”¯æŒåœ¨å•å¼ GPUä¸Šè¿è¡Œ
- **å¯æ‰©å±•**ï¼šæ”¯æŒå¢é‡æ›´æ–°çŸ¥è¯†åº“

### ğŸ”§ æ˜“äºä½¿ç”¨
- **å³æ’å³ç”¨**ï¼šä¸ç°æœ‰Transformersæ¨¡å‹æ— ç¼é›†æˆ
- **é…ç½®çµæ´»**ï¼šä¸°å¯Œçš„å‚æ•°è°ƒä¼˜é€‰é¡¹
- **å·¥å…·å®Œæ•´**ï¼šæä¾›å®Œæ•´çš„é¢„è®¡ç®—å’Œè¯„ä¼°å·¥å…·

## é…ç½®è°ƒä¼˜

### å…³é”®å‚æ•°è¯´æ˜
```python
model = CausalLlamaForCausalLM.from_pretrained_with_causal_editor(
    # åŸºç¡€é…ç½®
    model_name_or_path="meta-llama/Llama-2-7b-hf",
    vector_db_path="./path/to/vector_database",
    
    # ç¼–è¾‘å¼ºåº¦ï¼šæ§åˆ¶ä¿®æ­£å¹…åº¦ (0.1-3.0)
    edit_strength=1.0,
    
    # ç¼–è¾‘å±‚æ•°ï¼šå‚ä¸ç¼–è¾‘çš„é¡¶å±‚æ•°é‡ (5-20)
    top_layers=10,
    
    # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼šæ¿€æ´»æ£€ç´¢çš„æœ€ä½ç›¸ä¼¼åº¦ (0.5-0.9)
    similarity_threshold=0.8,
    
    # å†²çªé˜ˆå€¼ï¼šè§¦å‘ç¼–è¾‘çš„å†²çªç¨‹åº¦ (0.3-0.8)
    conflict_threshold=0.6
)
```

### æ€§èƒ½è°ƒä¼˜å»ºè®®
- **ç¼–è¾‘å¼ºåº¦**ï¼šä»1.0å¼€å§‹ï¼Œå¦‚æœä¿®æ­£ä¸è¶³å¯å¢åŠ åˆ°1.5-2.0
- **å±‚æ•°é€‰æ‹©**ï¼šæ¨¡å‹å±‚æ•°çš„1/3å·¦å³é€šå¸¸æ•ˆæœæœ€ä½³
- **é˜ˆå€¼è®¾ç½®**ï¼šé«˜é˜ˆå€¼=é«˜ç²¾åº¦ä½å¬å›ï¼Œä½é˜ˆå€¼=é«˜å¬å›ä½ç²¾åº¦

## é¡¹ç›®æ¶æ„

```
CausalEditor/
â”œâ”€â”€ causal_editor/                    # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ core/                        # æ ¸å¿ƒç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ causal_editor.py         # ä¸»è¦CausalEditorç±»
â”‚   â”‚   â”œâ”€â”€ conflict_detector.py     # å› æœå†²çªæ£€æµ‹å™¨
â”‚   â”‚   â”œâ”€â”€ counterfactual_editor.py # åäº‹å®ç¼–è¾‘å™¨
â”‚   â”‚   â””â”€â”€ vector_database.py       # å‘é‡æ•°æ®åº“ç®¡ç†
â”‚   â”œâ”€â”€ precompute/                  # é¢„è®¡ç®—æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ knowledge_extractor.py   # çŸ¥è¯†æå–
â”‚   â”‚   â”œâ”€â”€ fingerprint_builder.py   # æ¿€æ´»æŒ‡çº¹æ„å»º
â”‚   â”‚   â””â”€â”€ precompute_pipeline.py   # é¢„è®¡ç®—æµæ°´çº¿
â”‚   â””â”€â”€ utils/                       # å·¥å…·å‡½æ•°
â”œâ”€â”€ modeling_llms/                   # é›†æˆCausalEditorçš„æ¨¡å‹æ–‡ä»¶
â”‚   â””â”€â”€ modeling_llama_causal.py     # Llama + CausalEditor
â”œâ”€â”€ scripts/                         # è„šæœ¬æ–‡ä»¶
â”‚   â”œâ”€â”€ precompute_causal_knowledge.py  # é¢„è®¡ç®—è„šæœ¬
â”‚   â””â”€â”€ evaluate_truthfulqa_causal.py   # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ configs/                         # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ causal_editor_config.json    # é»˜è®¤é…ç½®
â””â”€â”€ test_data/                       # æµ‹è¯•æ•°æ®
```

## å¸¸è§ç”¨ä¾‹

### ç”¨ä¾‹1ï¼šç§‘å­¦äº‹å®ä¿®æ­£
```python
questions = [
    "What year was the theory of relativity published?",
    "Who discovered DNA structure?", 
    "When did World War II end?"
]

for q in questions:
    inputs = tokenizer(q, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=30)
    answer = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    print(f"Q: {q}")
    print(f"A: {answer}\n")
```

### ç”¨ä¾‹2ï¼šå¤šé€‰é¢˜è¯„ä¼°
```python
# è®¾ç½®å¤šé€‰é¢˜æ¨¡å¼
model.set_generation_mode(is_mc=True, prompt_length=len(question_tokens))

# è®¡ç®—ç­”æ¡ˆæ¦‚ç‡
prob_true = model.get_answer_probability(question, correct_answer)
prob_false = model.get_answer_probability(question, wrong_answer)
```

### ç”¨ä¾‹3ï¼šæ‰¹é‡å¤„ç†
```python
# æ‰¹é‡ç”Ÿæˆï¼ˆæ¨èæ–¹å¼ï¼‰
questions = ["question1", "question2", "question3"]
answers = model.batch_generate(questions, max_new_tokens=50)
```

## è¿›é˜¶åŠŸèƒ½

### è‡ªå®šä¹‰çŸ¥è¯†æº
```python
# ä½¿ç”¨CSVæ–‡ä»¶ä½œä¸ºçŸ¥è¯†æº
python scripts/precompute_causal_knowledge.py \
    --knowledge-type csv \
    --csv-path ./custom_knowledge.csv \
    --limit 5000
```

CSVæ ¼å¼è¦æ±‚ï¼š
```csv
subject,relation,object,confidence,text
Einstein,published_paper_on,Photoelectric Effect,1.0,Einstein published a paper on the photoelectric effect.
Einstein,published_year,1905,1.0,Einstein published his paper in 1905.
```

### å¢é‡æ›´æ–°
```python
# æ·»åŠ æ–°çŸ¥è¯†åˆ°ç°æœ‰æ•°æ®åº“
python scripts/precompute_causal_knowledge.py \
    --incremental \
    --existing-db-path ./precomputed_data/llama2-7b/vector_database \
    --knowledge-type csv \
    --csv-path ./new_knowledge.csv \
    --limit 1000
```

### ç»Ÿè®¡ç›‘æ§
```python
# è·å–å®æ—¶ç»Ÿè®¡
stats = model.get_causal_editor_statistics()
print(f"å†²çªæ£€æµ‹ç‡: {stats['conflict_detector_stats']['conflict_rate']:.3f}")
print(f"ç¼–è¾‘æˆåŠŸç‡: {stats['counterfactual_editor_stats']['success_rate']:.3f}")
print(f"å¹³å‡ç¼–è¾‘å¹…åº¦: {stats['counterfactual_editor_stats']['average_edit_magnitude']:.3f}")
```

## é—®é¢˜æ’æŸ¥

### Q: é¢„è®¡ç®—é˜¶æ®µæ˜¾å­˜ä¸è¶³ï¼Ÿ
```bash
# é™ä½æ‰¹å¤„ç†å¤§å°
python scripts/precompute_causal_knowledge.py --batch-size 4

# æˆ–ä½¿ç”¨CPUï¼ˆè¾ƒæ…¢ï¼‰
python scripts/precompute_causal_knowledge.py --device cpu
```

### Q: Wikidataè®¿é—®è¶…æ—¶ï¼Ÿ
```bash
# å¢åŠ è¯·æ±‚é—´éš”
python scripts/precompute_causal_knowledge.py --rate-limit 2.0

# æˆ–ä½¿ç”¨æœ¬åœ°CSVæ–‡ä»¶
python scripts/precompute_causal_knowledge.py --knowledge-type csv --csv-path ./local_data.csv
```

### Q: ç¼–è¾‘æ•ˆæœä¸æ˜æ˜¾ï¼Ÿ
```python
# å¢åŠ ç¼–è¾‘å¼ºåº¦
model = CausalLlamaForCausalLM.from_pretrained_with_causal_editor(
    # ... å…¶ä»–å‚æ•°
    edit_strength=1.5,  # ä»1.0å¢åŠ åˆ°1.5
    top_layers=15       # å¢åŠ ç¼–è¾‘å±‚æ•°
)
```

### Q: æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Ÿ
```python
# å‡å°‘ç¼–è¾‘å±‚æ•°
model = CausalLlamaForCausalLM.from_pretrained_with_causal_editor(
    # ... å…¶ä»–å‚æ•°
    top_layers=5,                    # å‡å°‘å±‚æ•°
    similarity_threshold=0.9         # æé«˜é˜ˆå€¼ï¼Œå‡å°‘æ£€ç´¢
)
```

## å®éªŒè®¾è®¡

### è¯„ä¼°æŒ‡æ ‡
1. **TruthfulQA**:
   - MC1/MC2å‡†ç¡®ç‡ï¼ˆå¤šé€‰é¢˜ï¼‰
   - True*Infoè¯„åˆ†ï¼ˆç”Ÿæˆä»»åŠ¡ï¼‰

2. **æ•ˆç‡æŒ‡æ ‡**:
   - æ¨ç†å»¶è¿Ÿ
   - æ˜¾å­˜å¼€é”€
   - å†²çªæ£€æµ‹ç‡

3. **è´¨é‡æŒ‡æ ‡**:
   - äº‹å®å‡†ç¡®æ€§æ”¹è¿›
   - è¯­è¨€æµç•…åº¦ä¿æŒ
   - å‰¯ä½œç”¨åˆ†æ

### å¯¹æ¯”æ–¹æ³•
- **Baseline**: åŸå§‹LLM
- **TruthX**: ç»Ÿä¸€çœŸå®æ€§æ–¹å‘ç¼–è¾‘
- **RAG**: æ£€ç´¢å¢å¼ºç”Ÿæˆ
- **CausalEditor**: æœ¬æ–¹æ³•

## æ¶ˆèå®éªŒ

### RQ1: ç»„ä»¶æœ‰æ•ˆæ€§
```bash
# ç¦ç”¨ç¼–è¾‘ï¼Œä»…æ£€æµ‹
python scripts/evaluate_truthfulqa_causal.py \
    --edit-strength 0.0  # éªŒè¯æ£€æµ‹æœºåˆ¶çš„æœ‰æ•ˆæ€§
```

### RQ2: å‚æ•°æ•æ„Ÿæ€§
```bash
# ä¸åŒç¼–è¾‘å¼ºåº¦
for strength in 0.5 1.0 1.5 2.0; do
    python scripts/evaluate_truthfulqa_causal.py \
        --edit-strength $strength \
        --output-dir ./results/strength_$strength
done

# ä¸åŒå±‚æ•°
for layers in 5 10 15 20; do
    python scripts/evaluate_truthfulqa_causal.py \
        --top-layers $layers \
        --output-dir ./results/layers_$layers
done
```

### RQ3: ç²¾å‡†æ€§åˆ†æ
- åˆ†æç¼–è¾‘çš„å…·ä½“ä½ç½®å’Œå¹…åº¦
- å¯¹æ¯”CausalEditorä¸RAGçš„è¾“å‡ºå·®å¼‚
- è¯„ä¼°"å¤–ç§‘æ‰‹æœ¯å¼"ç¼–è¾‘çš„ç²¾ç¡®åº¦

## è®ºæ–‡å®éªŒå¤ç°

### å¤ç°ä¸»å®éªŒ
```bash
# 1. é¢„è®¡ç®—ï¼ˆçº¦2-4å°æ—¶ï¼‰
python scripts/precompute_causal_knowledge.py \
    --model-name meta-llama/Llama-2-7b-hf \
    --limit 10000 \
    --output-dir ./precomputed_data/llama2-7b

# 2. TruthfulQAè¯„ä¼°ï¼ˆçº¦1-2å°æ—¶ï¼‰  
python scripts/evaluate_truthfulqa_causal.py \
    --model-path meta-llama/Llama-2-7b-hf \
    --vector-db-path ./precomputed_data/llama2-7b/vector_database \
    --output-dir ./results/truthfulqa_main \
    --mode both

# 3. æŸ¥çœ‹ç»“æœ
cat ./results/truthfulqa_main/evaluation_summary.json
```

### é¢„æœŸæ”¹è¿›æ•ˆæœ
- **TruthfulQA MC1å‡†ç¡®ç‡**: +15~25%
- **TruthfulQA MC2å‡†ç¡®ç‡**: +10~20%  
- **ç”Ÿæˆè´¨é‡**: æ˜¾è‘—å‡å°‘äº‹å®æ€§é”™è¯¯
- **æ¨ç†å»¶è¿Ÿ**: å¢åŠ <10%

## æ ¸å¿ƒç»„ä»¶è¯´æ˜

### é¢„è®¡ç®—å·¥å…·
- `WikidataExtractor`: ä»Wikidataæå–é«˜è´¨é‡çŸ¥è¯†ä¸‰å…ƒç»„
- `ActivationFingerprintBuilder`: æ„å»ºLLMæ¿€æ´»æŒ‡çº¹ï¼Œæ”¯æŒå¤šå±‚å¹¶è¡Œ
- `PrecomputePipeline`: å®Œæ•´çš„é¢„è®¡ç®—æµæ°´çº¿ï¼Œæ”¯æŒå¢é‡æ›´æ–°

### æ ¸å¿ƒç»„ä»¶
- `CausalEditor`: ä¸»æ§åˆ¶å™¨ï¼Œæ•´åˆæ‰€æœ‰ç»„ä»¶
- `CausalConflictDetector`: æ£€æµ‹å› æœå†²çªï¼Œæ”¯æŒå®ä½“è¯†åˆ«å’ŒçŸ¥è¯†æ£€ç´¢
- `CounterfactualEditor`: æ‰§è¡Œåäº‹å®ç¼–è¾‘ï¼Œè®¡ç®—æœ€ä¼˜ç¼–è¾‘æ–¹å‘
- `VectorDatabase`: FAISSå‘é‡æ•°æ®åº“ç®¡ç†ï¼Œæ”¯æŒGPUåŠ é€Ÿ

### æ¨¡å‹é›†æˆ
- `CausalLlamaForCausalLM`: é›†æˆCausalEditorçš„Llamaæ¨¡å‹
- æ”¯æŒå¤šé€‰é¢˜å’Œç”Ÿæˆä»»åŠ¡çš„æ¨¡å¼åˆ‡æ¢
- å…¼å®¹Transformersåº“çš„æ ‡å‡†æ¥å£

## æ³¨æ„äº‹é¡¹

1. **æ˜¾å­˜éœ€æ±‚**: é¢„è®¡ç®—é˜¶æ®µéœ€è¦è¾ƒå¤§æ˜¾å­˜ï¼Œå»ºè®®24GBä»¥ä¸Š
2. **æ—¶é—´å¼€é”€**: é¦–æ¬¡é¢„è®¡ç®—å¯èƒ½éœ€è¦æ•°å°æ—¶ï¼Œå»ºè®®ä½¿ç”¨ç¼“å­˜
3. **æ¨¡å‹å…¼å®¹**: ç›®å‰ä¸»è¦æ”¯æŒLlamaç³»åˆ—ï¼Œå…¶ä»–æ¨¡å‹éœ€è¦é€‚é…
4. **æ•°æ®è´¨é‡**: çŸ¥è¯†ä¸‰å…ƒç»„çš„è´¨é‡ç›´æ¥å½±å“ç¼–è¾‘æ•ˆæœ

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†CausalEditorï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{causaleditor2024,
    title={CausalEditor: å±‚çº§åŒ–å› æœæº¯æºä¸åäº‹å®ç¼–è¾‘},
    author={CausalEditor Team},
    year={2024},
    note={åŸºäºå±‚çº§åŒ–å› æœæº¯æºçš„LLMå¹»è§‰ä¿®æ­£æ–¹æ³•}
}
``` 